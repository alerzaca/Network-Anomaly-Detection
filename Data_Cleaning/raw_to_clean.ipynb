{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "arOcMFMUyTKT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from fastai.tabular.all import df_shrink"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cLhwCLGQ66N"
      },
      "source": [
        "### Reading local files\n",
        "The CSV files containing the data are too large to be uploaded to GitHub. All sources are linked in the README."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igRujRl18tUM",
        "outputId": "b24f983c-b60b-4b8b-d45e-d7932f068b63"
      },
      "outputs": [],
      "source": [
        "dspaths = []\n",
        "\n",
        "def read_paths(dir_path):\n",
        "  for dirname, _, filenames in os.walk(dir_path):\n",
        "      for filename in filenames:\n",
        "          if filename.endswith('.csv'):\n",
        "              pds = os.path.join(dirname, filename)\n",
        "              dspaths.append(pds)\n",
        "              print(pds)\n",
        "  return dspaths\n",
        "\n",
        "def read_files(dspaths):\n",
        "  individual_dfs = [pd.read_csv(dsp) for dsp in dspaths]\n",
        "  [i.shape for i in individual_dfs]\n",
        "  return individual_dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "../Local/2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "../Local/2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "../Local/2017/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "../Local/2017/Monday-WorkingHours.pcap_ISCX.csv\n",
            "../Local/2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "../Local/2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "../Local/2017/Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "../Local/2017/Wednesday-workingHours.pcap_ISCX.csv\n"
          ]
        }
      ],
      "source": [
        "dspath2017 = read_paths('../Local/2017/')\n",
        "dfs2017 = read_files(dspath2017)\n",
        "\n",
        "# Warning: more than 30 GB of RAM is needed to run this code \n",
        "#dspath2018 = read_paths('../Local/2018/')\n",
        "#dfs2018 = read_files(dspath2018)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              " Destination Port                int64\n",
              " Flow Duration                   int64\n",
              " Total Fwd Packets               int64\n",
              " Total Backward Packets          int64\n",
              "Total Length of Fwd Packets      int64\n",
              "                                ...   \n",
              "Idle Mean                      float64\n",
              " Idle Std                      float64\n",
              " Idle Max                        int64\n",
              " Idle Min                        int64\n",
              " Label                          object\n",
              "Length: 79, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfs2017[0].dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downsizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eOa1fdEKk95H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " Destination Port                 int32\n",
              " Flow Duration                    int32\n",
              " Total Fwd Packets                int16\n",
              " Total Backward Packets           int16\n",
              "Total Length of Fwd Packets       int32\n",
              "                                 ...   \n",
              "Idle Mean                       float32\n",
              " Idle Std                       float32\n",
              " Idle Max                         int32\n",
              " Idle Min                         int32\n",
              " Label                         category\n",
              "Length: 79, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfs2017 = [df_shrink(df) for df in dfs2017]\n",
        "dfs2017[0].dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-6AIu_SX4Py"
      },
      "source": [
        "### Deleting NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aL-VFcA4lzvJ"
      },
      "outputs": [],
      "source": [
        "def drop_nan(individual_dfs):\n",
        "  for df in individual_dfs:\n",
        "      df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "#      print(df.isna().any(axis=1).sum(), \"rows with at least one NaN to remove\")\n",
        "      df.dropna(inplace=True)\n",
        "  [i.shape for i in individual_dfs]\n",
        "  return individual_dfs\n",
        "\n",
        "dfs2017 = drop_nan(dfs2017)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwRNwmvMYWO2"
      },
      "source": [
        "### Deleting duplicate rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvFU_7el5jW",
        "outputId": "d474c5f0-012d-4ec8-9d04-dfe5f3b2e443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2629 fully duplicate rows to remove\n",
            "72319 fully duplicate rows to remove\n",
            "6867 fully duplicate rows to remove\n",
            "26831 fully duplicate rows to remove\n",
            "35605 fully duplicate rows to remove\n",
            "6052 fully duplicate rows to remove\n",
            "24019 fully duplicate rows to remove\n",
            "80914 fully duplicate rows to remove\n"
          ]
        }
      ],
      "source": [
        "def drop_dupes(individual_dfs):\n",
        "  for df in individual_dfs:\n",
        "#      print(df.duplicated().sum(), \"fully duplicate rows to remove\")\n",
        "      df.drop_duplicates(inplace=True)\n",
        "      df.reset_index(inplace=True, drop=True)\n",
        "  [i.shape for i in individual_dfs]\n",
        "  return individual_dfs\n",
        "\n",
        "dfs2017 = drop_dupes(dfs2017)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7FIVhketrm"
      },
      "source": [
        "### Saving data back into CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "svDHjAKptCmB"
      },
      "outputs": [],
      "source": [
        "def convert(individual_dfs):\n",
        "  for i, df in enumerate(individual_dfs):\n",
        "        csv_path = f\"../IDS2017/Clean/{dspaths[i].split('/')[-1].replace('.pcap_ISCX', '')[0:]}\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "\n",
        "convert(dfs2017)\n",
        "\n",
        "combined_df = pd.concat(dfs2017, ignore_index=True)\n",
        "combined_df.to_csv(\"../IDS2017/Clean/Combined.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRUiNgUJZJaP"
      },
      "source": [
        "### Converting data to parquet format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jWNmfvnWmD44"
      },
      "outputs": [],
      "source": [
        "def convert(individual_dfs):\n",
        "  for i, df in enumerate(individual_dfs):\n",
        "      df.to_parquet(f\"../IDS2017/Clean/{dspaths[i].split('/')[-1].replace('.pcap_ISCX.csv', '.parquet')[0:]}\")\n",
        "\n",
        "convert(dfs2017)\n",
        "\n",
        "def convert_to_single_parquet(individual_dfs, output_path):\n",
        "    combined_df = pd.concat(individual_dfs, ignore_index=True)\n",
        "    combined_df.to_parquet(output_path)\n",
        "\n",
        "convert_to_single_parquet(dfs2017, \"../IDS2017/Clean/Combined.parquet\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
